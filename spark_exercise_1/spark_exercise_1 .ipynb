{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a7cd400-cf20-4fb6-9e18-537e3477ee68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div  align='center'><img src='https://s3.amazonaws.com/weclouddata/images/logos/wcd_logo_new_2.png' width='30%'></div >\n",
    "\n",
    "<p style=\"font-size:20px;text-align:center\"><b><font color='#F39A54'>Data Engineering Diploma</font></b></p>\n",
    "\n",
    "<h2 align='center'> WeCloudData Data Engineer Spark Exercise 1 </h2>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69193995-8250-49e9-afaf-58d06ecf27f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Please download data from [HERE](s3://weclouddata/data/data/pyspark_exercises_data.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa6f59e-df18-40f6-8828-d2a8028ebbe6",
     "showTitle": false,
     "title": ""
    },
    "id": "p9rHQVQXA2gV"
   },
   "source": [
    "# Section 1. Reading, Writing and Validating Data in PySpark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00265db-b964-4f28-a808-314771d7ab39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to always begin every Spark session by creating a Spark instance. Let's go ahead and use the method we learned in the lecture in the cell below. Also see if you can remember how to open the Spark UI (using a link that automatically guides you there). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f0c9d2c-d886-4f2e-af2e-abdc2e2063c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72424107-15cb-4e54-9e6d-819e2305a4d3",
     "showTitle": false,
     "title": ""
    },
    "id": "g_12i0vsA2ga"
   },
   "source": [
    "## Next let's start by reading a basic csv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d039f67-f723-4ab0-8880-395cc3095dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Download the pga_tour_historical dataset that is attached to this lecture and save it whatever folder you want, then read it in. \n",
    "\n",
    "**Data Source:** `pga-tour-historical.csv`\n",
    "\n",
    "Rememer to try letting Spark infer the header and infer the Schema types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d20c2916-a379-4edc-bcc0-1a44065e6568",
     "showTitle": false,
     "title": ""
    },
    "id": "3eufR0XoA2ga"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966366b8-7ee9-4ef9-aea9-e101106ddb9e",
     "showTitle": false,
     "title": ""
    },
    "id": "33btaZAPA2gc"
   },
   "source": [
    "## S1.1. View first 5 lines of dataframe\n",
    "First generate a view of the first 5 lines of the dataframe to get an idea of what is inside. We went over two ways of doing this... see if you can remember BOTH ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7732be51-3a7e-43cc-b6f4-67375d91b7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58cc5e20-332c-495d-ab0d-367e07880ca9",
     "showTitle": false,
     "title": ""
    },
    "id": "rpLiYlvSA2ge"
   },
   "source": [
    "## S1.2. Print the schema details\n",
    "\n",
    "Now print the details of the dataframes schema that Spark infered to ensure that it was infered correctly. Sometimes it is not infered correctly, so we need to watch out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a43be9-b1f2-44e2-ade5-197ed2cd831c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37332a78-f136-40f1-8621-c4cada79d78c",
     "showTitle": false,
     "title": ""
    },
    "id": "uZgyg7z9A2gf"
   },
   "source": [
    "## S1.3. Edit the schema during the read in\n",
    "\n",
    "We can see from the output above that Spark did not correctly infer that the \"value\" column was an integer value. Let's try specifying the schema this time to let spark know what the schema should be.\n",
    "\n",
    "Here is a link to see a list of PySpark data types in case you need it (also attached to the lecture): \n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a79633-0760-44ce-8d13-621fe8288176",
     "showTitle": false,
     "title": ""
    },
    "id": "EqEF2tlUA2gg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05d6d93-9cb5-4f43-8740-44bf6ab5b338",
     "showTitle": false,
     "title": ""
    },
    "id": "7yjcI60kA2gj"
   },
   "source": [
    "## S1.4. Generate summary statistics for only one variable\n",
    "See if you can generate summary statistics for only the \"Value\" column using the .describe function\n",
    "\n",
    "(count, mean, stddev, min, max) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f217487a-3a62-446e-af12-891183b3cb5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e4cfb0-6b6f-4e3d-a43a-105be3d1ff9a",
     "showTitle": false,
     "title": ""
    },
    "id": "nN2UCIWXA2gk"
   },
   "source": [
    "## S1.5. Generate summary statistics for TWO variables\n",
    "Now try to generate ONLY the count min and max for BOTH the \"Value\" and \"Season\" variable using the select. You can't use the .describe function for this one but see if you can remember which function you CAN use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc1194c-74cf-4384-a566-79bc5bf918ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae3498e-6d71-4bf1-af48-054fca14baad",
     "showTitle": false,
     "title": ""
    },
    "id": "nhJ3aRXoA2gl"
   },
   "source": [
    "## S1.6. Write a parquet file\n",
    "\n",
    "Now try writing a parquet file (not partitioned) from the pga dataset. But first create a new dataframe containing ONLY the the \"Season\" and \"Value\" fields (using the \"select command you used in the question above) and write a parquet file partitioned by \"Season\". This is a bit of a challenge aimed at getting you ready for material that will be covered later on in the course. Don't feel bad if you can't figure it out.\n",
    "\n",
    "*Note that if any of your variable names contain spaces, spark will produce an error message with this call. That is why we are selecting ONLY the \"Season\" and \"Value\" fields. Ideally we should renamed those columns but we haven't gotten to that yet in this course but we will soon!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269f083f-b63f-450f-bd4c-3d39e9bd6e98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf86230-f6d5-4906-9ad5-b4986c7fcece",
     "showTitle": false,
     "title": ""
    },
    "id": "iMCN1CRWA2gm"
   },
   "source": [
    "## S1.7. Write a partitioned parquet file\n",
    "\n",
    "You will need to use the same limited dataframe that you created in the previous question to accomplish this task as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d30dd9e-beb3-4a6f-9f11-9c9de59cd224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc56e72-eefd-4a75-89b9-a8c9c4ce5937",
     "showTitle": false,
     "title": ""
    },
    "id": "hj7-W-nDA2gn"
   },
   "source": [
    "## S1.8. Read in a partitioned parquet file\n",
    "\n",
    "Now try reading in the partitioned parquet file you just created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbd4d9d7-e3fd-418f-891a-c506ef3c1bea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14314053-211f-4496-9220-9c788314b77d",
     "showTitle": false,
     "title": ""
    },
    "id": "X6PIYDuUA2go"
   },
   "source": [
    "## S1.9. Reading in a set of paritioned parquet files\n",
    "\n",
    "Now try only reading Seasons 2010, 2011 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e70bc6-7ed2-4ee2-b048-ccc9b0fac649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f4d243-d3ac-4ed0-aa02-2eb0ca40d789",
     "showTitle": false,
     "title": ""
    },
    "id": "to1ULOa-A2gp"
   },
   "source": [
    "## S1.10. Create your own dataframe\n",
    "\n",
    "Try creating your own dataframe below using PySparks *.createDataFrame* function. See if you can make one that contains 4 variables and at least 3 rows. \n",
    "\n",
    "Let's see how creative you can get on the content of the dataframe :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad67008-3205-4d9d-88d9-6e3ad5f8d6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeaaa1c6-4101-4bc6-aba2-84f65ad21ad2",
     "showTitle": false,
     "title": ""
    },
    "id": "vF0inwX4A2gq"
   },
   "source": [
    "# Section 2. Manipulating Data in DataFrames\n",
    "\n",
    "#### Let's get started applying what we learned in the lecure!\n",
    "\n",
    "I've provided several questions below to help test and expand you knowledge from the code along lecture. So let's see what you've got!\n",
    "\n",
    "First create your spark instance as we need to do at the start of every project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81547f8-f257-4744-8598-12486f7432ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "137efd22-510b-49f1-a67a-899a2b6792da",
     "showTitle": false,
     "title": ""
    },
    "id": "r70W8E7dA2gr"
   },
   "source": [
    "## Read in our Republican vs. Democrats Tweet DataFrame\n",
    "### About this dataframe\n",
    "\n",
    "Extracted tweets from all of the representatives (latest 200 as of May 17th 2018)\n",
    "\n",
    "**Source:** `Rep_vs_Dem_tweets.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4cc583-0120-4e77-8035-1ec573cc8e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81bd6bb-ac6e-4a8b-af53-f57ff04c8383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use either .show() or .toPandas() check out the first view rows of the dataframe to get an idea of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60bd8054-5d20-42c1-b6eb-ef172197c057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b930e6ad-af77-4cf3-ad84-735b9f4b5382",
     "showTitle": false,
     "title": ""
    },
    "id": "WHM78z3dA2gt"
   },
   "source": [
    "**Prevent Truncation of view**\n",
    "\n",
    "If the view you produced above truncated some of the longer tweets, see if you can prevent that so you can read the whole tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cea3675-d7e7-44c2-9bb3-2097d00bbb2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a0cfd08-b6fd-4b39-9111-e68f57adf1d6",
     "showTitle": false,
     "title": ""
    },
    "id": "lh7bTNlrA2gt"
   },
   "source": [
    "**Print Schema**\n",
    "\n",
    "First, check the schema to make sure the datatypes are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354e4016-983e-4165-96ae-399ffe1bce1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8857e5-23e2-45fc-aca1-f06a0a35073c",
     "showTitle": false,
     "title": ""
    },
    "id": "_dQLhQZOA2gu"
   },
   "source": [
    "## S2.1. Can you identify any tweet that mentions the handle @LatinoLeader using regexp_extract?\n",
    "\n",
    "It doesn't matter how you identify the row, any identifier will do. You can test your script on row 5 from this dataset. That row contains @LatinoLeader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e59f3e-255c-4c42-abc3-5dbd18d8ce48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb42fcd-dcf2-4e35-a3ec-98736fb460cf",
     "showTitle": false,
     "title": ""
    },
    "id": "eqyTvWaCA2gv"
   },
   "source": [
    "## S2.2. Replace any value other than 'Democrate' or 'Republican' with 'Other' in the Party column.\n",
    "\n",
    "We can see from the output below, that there are several other values other than 'Democrate' or 'Republican' in the Part column. We are assuming that this is dirty data that needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c792bb76-2ee4-435e-a8b9-bbe24d46c61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867adc32-634b-4e5c-93f2-77b7bb20f6bb",
     "showTitle": false,
     "title": ""
    },
    "id": "dyK1rUWvA2gw"
   },
   "source": [
    "## S2.3. Delete all embedded links (ie. \"https:....)\n",
    "\n",
    "For example see the first row in the tweets dataframe. \n",
    "\n",
    "*Note: this may require an google search :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b691ed8c-7c01-4fca-8a07-0ac9a8872382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a43aafbb-ac6f-43fc-9d53-66c2b9435844",
     "showTitle": false,
     "title": ""
    },
    "id": "yuKE3lw2A2gx"
   },
   "source": [
    "## S2.4. Remove any leading or trailing white space in the tweet column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de0f003-87f4-4097-9b39-9df883f91166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa40b285-a5da-4432-9f07-4f4910ea87a0",
     "showTitle": false,
     "title": ""
    },
    "id": "7P7_QAZRA2gy"
   },
   "source": [
    "## S2.5. Rename the 'Party' column to 'Dem_Rep'\n",
    "\n",
    "No real reason here :) just wanted you to get practice doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d0e307-d004-47a6-8d2b-9ea4bd2a4bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a743178-cac8-448e-a1ae-cd99c7fd2ec3",
     "showTitle": false,
     "title": ""
    },
    "id": "V5AO-dYLA2gz"
   },
   "source": [
    "## S2.6. Concatenate the Party and Handle columns\n",
    "\n",
    "Silly yes... but good practice.\n",
    "\n",
    "pyspark.sql.functions.concat_ws(sep, *cols)[source](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.concat_ws.html)\n",
    "\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da07d239-5015-4226-bbb1-f861129eea68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff1ebdab-d928-43e2-8b4b-505337404674",
     "showTitle": false,
     "title": ""
    },
    "id": "jBsqSTAdA2g0"
   },
   "source": [
    "## Challenge Question\n",
    "\n",
    "Let's image that we want to analyze the hashtags that are used in these tweets. Can you extract all the hashtags you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5284b30a-ee72-45c6-a7a8-1dbeb284a14f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed04573-e036-49d2-a2b6-0f2d6bf8c031",
     "showTitle": false,
     "title": ""
    },
    "id": "5SB47NlpA2g0"
   },
   "source": [
    "## Let's create our own dataset to work with real dates\n",
    "\n",
    "This is a dataset of patient visits from a medical office. It contains the patients first and last names, date of birth, and the dates of their first 3 visits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8106c8-1a57-47a3-ba59-cd005d67e3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "md_office = [('Mohammed','Alfasy','1987-4-8','2016-1-7','2017-2-3','2018-3-2') \\\n",
    "            ,('Marcy','Wellmaker','1986-4-8','2015-1-7','2017-1-3','2018-1-2') \\\n",
    "            ,('Ginny','Ginger','1986-7-10','2014-8-7','2015-2-3','2016-3-2') \\\n",
    "            ,('Vijay','Doberson','1988-5-2','2016-1-7','2018-2-3','2018-3-2') \\\n",
    "            ,('Orhan','Gelicek','1987-5-11','2016-5-7','2017-1-3','2018-9-2') \\\n",
    "            ,('Sarah','Jones','1956-7-6','2016-4-7','2017-8-3','2018-10-2') \\\n",
    "            ,('John','Johnson','2017-10-12','2018-1-2','2018-10-3','2018-3-2') ]\n",
    "\n",
    "df = spark.createDataFrame(md_office,['first_name','last_name','dob','visit1','visit2','visit3']) # schema=final_struc\n",
    "\n",
    "# Check to make sure it worked\n",
    "df.show()\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11977732-4aee-4f7f-98b8-0e22ada7b978",
     "showTitle": false,
     "title": ""
    },
    "id": "tSsQXDdIA2g1"
   },
   "source": [
    "Oh no! The dates are still stored as text... let's try converting them again and see if we have any issues this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccf612a-7328-4924-81fc-c13228b57a0d",
     "showTitle": false,
     "title": ""
    },
    "id": "x499Xq8EA2g2"
   },
   "source": [
    "## S2.7. Can you calculate a variable showing the length of time between patient visits?\n",
    "\n",
    "Compare visit1 to visit2 and visit2 to visit3 for all patients and see what the average length of time is between visits. Create an alias for it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0becc232-1f7e-4051-a9a0-8027071050d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f542685-2527-434d-a5d1-1caf2ce2f3fd",
     "showTitle": false,
     "title": ""
    },
    "id": "kwg19-gAA2g2"
   },
   "source": [
    "## S2.8. Can you calculate the age of each patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98e64529-e2f2-49e8-917d-212ed77843ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1aa99d0-34d4-45be-b0ed-1b31bb5297f4",
     "showTitle": false,
     "title": ""
    },
    "id": "dmAm15YBA2g3"
   },
   "source": [
    "## S2.9. Can you extract the month from the first visit column and call it \"Month\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e719a11-b58d-410f-b7db-3d30d21257b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d8cb23-30e7-46e5-92ca-96db506e8b2e",
     "showTitle": false,
     "title": ""
    },
    "id": "qLAGGIB9A2g5"
   },
   "source": [
    "## S2.10. Challenges with working with date and timestamps\n",
    "\n",
    "Let's read in the supermarket sales data `supermarket_sales.csv` to see some of the issues that can come up when working with date and timestamps values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06974610-9427-4593-b602-193c4939427c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be198eed-08ab-451e-a468-4f577ac2e304",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### About this dataset\n",
    "\n",
    "The growth of supermarkets in most populated cities are increasing and market competitions are also high. The dataset is one of the historical sales of supermarket company which has recorded in 3 different branches for 3 months data. \n",
    "\n",
    " - Attribute information\n",
    " - Invoice id: Computer generated sales slip invoice identification number\n",
    " - Branch: Branch of supercenter (3 branches are available identified by A, B and C).\n",
    " - City: Location of supercenters\n",
    " - Customer type: Type of customers, recorded by Members for customers using member card and Normal for without member card.\n",
    " - Gender: Gender type of customer\n",
    " - Product line: General item categorization groups - Electronic accessories, Fashion accessories, Food and beverages, Health and beauty, Home and lifestyle, Sports and travel\n",
    " - Unit price: Price of each product in USD\n",
    " - Quantity: Number of products purchased by customer\n",
    " - Tax: 5% tax fee for customer buying\n",
    " - Total: Total price including tax\n",
    " - Date: Date of purchase (Record available from January 2019 to March 2019)\n",
    " - Time: Purchase time (10am to 9pm)\n",
    " - Payment: Payment used by customer for purchase (3 methods are available â€“ Cash, Credit card and Ewallet)\n",
    " - COGS: Cost of goods sold\n",
    " - Gross margin percentage: Gross margin percentage\n",
    " - Gross income: Gross income\n",
    " - Rating: Customer stratification rating on their overall shopping experience (On a scale of 1 to 10)\n",
    "\n",
    "**Source:** https://www.kaggle.com/aungpyaeap/supermarket-sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e33e5d3a-337f-44eb-8c27-d3eada4709ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### View dataframe and schema as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9443c8eb-b4e3-422d-b173-5f69b9e2b287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3923bf8-820e-4e07-b44d-2453d45d4d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "403d42df-f58a-4877-8562-0debfcb5237d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert date field to date type\n",
    "\n",
    "Looks like we need to convert the date field into a date type. Let's go ahead and do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c96f215-a13c-4320-90ee-f5aa23c8a6fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55bdf990-98cd-44d2-9776-715efb86365b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### How can we extract the month value from the date field?\n",
    "\n",
    "If you had trouble converting the date field in the previous question think about a more creative solution to extract the month from that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a301316-2bee-45be-8e48-a102c6ed8306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "424edd92-0bd3-4d28-b3e6-ad61bba0edda",
     "showTitle": false,
     "title": ""
    },
    "id": "BUaNHlgvA2g7"
   },
   "source": [
    "## S2.11.0 Working with Arrays\n",
    "\n",
    "Here is a dataframe of reviews from the movie the Dark Night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1318c2dc-c391-4d83-9aa2-7f4526abc7cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "values = [(5,'Epic. This is the best movie I have EVER seen'), \\\n",
    "          (4,'Pretty good, but I would have liked to seen better special effects'), \\\n",
    "          (3,'So so. Casting could have been improved'), \\\n",
    "          (5,'The most EPIC movie of the year! Casting was awesome. Special effects were so intense.'), \\\n",
    "          (4,'Solid but I would have liked to see more of the love story'), \\\n",
    "          (5,'THE BOMB!!!!!!!')]\n",
    "reviews = spark.createDataFrame(values,['rating', 'review_txt'])\n",
    "\n",
    "reviews.show(6,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202e8ddb-9e5b-4ddd-a53a-abe5046fa67d",
     "showTitle": false,
     "title": ""
    },
    "id": "Y40EFu3qA2g8"
   },
   "source": [
    "## S2.11.1 Let's see if we can create an array off of the review text column and then derive some meaningful results from it.\n",
    "\n",
    "**But first** we need to clean the rview_txt column to make sure we can get what we need from our analysis later on. So let's do the following:\n",
    "\n",
    "1. Remove all punctuation\n",
    "2. lower case everything\n",
    "3. Remove white space (trim)\n",
    "3. Then finally, split the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d19b1c8-a6f8-4bc4-8185-262dcc3a5c0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f503b4-6e49-4737-baf0-9150daa87ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f948fa4-4cfd-480b-afec-ea4cf0972f07",
     "showTitle": false,
     "title": ""
    },
    "id": "2cg5hCSJA2g9"
   },
   "source": [
    "## S2.11.2 Alright now let's see if we can find which reviews contain the word 'Epic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee3dd28b-d7ed-47cf-975c-0064d2a61b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8fd07e-e73c-406a-ab9b-e99dbe773c20",
     "showTitle": false,
     "title": ""
    },
    "id": "3U15qoh2A2g-"
   },
   "source": [
    "# Section 3. Handling Missing Data in PySpark\n",
    "\n",
    "You will be strengthening your skill sets dealing with missing data.\n",
    " \n",
    "**Review:** you have 2 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "1. Drop the missing data points (including the entire row)\n",
    "2. Fill them in with some other value.\n",
    "\n",
    "Let's practice some examples of each of these methods!\n",
    "\n",
    "\n",
    "#### But first!\n",
    "\n",
    "Start your Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cbcc836-4833-4c50-b5cc-cc92fd1e6de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3a5c402-5897-47d3-a919-65c9431cd5a9",
     "showTitle": false,
     "title": ""
    },
    "id": "ljYy8sXzA2g_"
   },
   "source": [
    "## Read in the dataset for this Notebook\n",
    "**Data Source**: `Weather.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a82886-e543-4d57-8094-9bd370ab022c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517908d6-04ff-4d72-9531-5fa80fa430df",
     "showTitle": false,
     "title": ""
    },
    "id": "r_NS6HTYA2g_"
   },
   "source": [
    "## About this dataset\n",
    "\n",
    "**New York City Taxi Trip - Hourly Weather Data**\n",
    "\n",
    "Here is some detailed weather data for the New York City Taxi Trips.\n",
    "\n",
    "**Source:** https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "719e8476-2f5a-49c0-a2d8-dec406e141e8",
     "showTitle": false,
     "title": ""
    },
    "id": "jLSTdRTbA2g_"
   },
   "source": [
    "### Print a view of the first several lines of the dataframe to see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056ffc9c-aacc-4349-a95d-accb21627a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eed4819-7cdd-4288-855f-3985cb78d939",
     "showTitle": false,
     "title": ""
    },
    "id": "l4YwX25GA2hA"
   },
   "source": [
    "### Print the schema \n",
    "\n",
    "So that we can see if we need to make any corrections to the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb493b6-745a-468f-a718-4e385344ffb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7805dd6f-e1b5-4ca7-8eb5-5ed6fed5bbca",
     "showTitle": false,
     "title": ""
    },
    "id": "GEJTRzJQA2hA"
   },
   "source": [
    "## S3.1. How much missing data are we working with?\n",
    "\n",
    "Get a count and percentage of each variable in the dataset to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04732ca-1335-4f4f-b835-4a799dd4813b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109ef792-aab7-4eb6-a047-c7507207d6f9",
     "showTitle": false,
     "title": ""
    },
    "id": "OsYoNIvMA2hB"
   },
   "source": [
    "## S3.2. How many rows contain at least one null value?\n",
    "\n",
    "We want to know, if we use the df.ha option, how many rows will we loose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03525ab6-95a6-47b6-9a40-8f09fcecdce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f519945f-20ff-4bba-89c4-820f9ce61994",
     "showTitle": false,
     "title": ""
    },
    "id": "OlTSFZiIA2hC"
   },
   "source": [
    "## S3.3. Drop the missing data\n",
    "\n",
    "Drop any row that contains missing data across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2686967-a5bc-49bb-bacf-870cfc423f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ce09c0-95a9-4d8a-b2a3-308d8f02e45c",
     "showTitle": false,
     "title": ""
    },
    "id": "V__PC30YA2hD"
   },
   "source": [
    "## S3.4. Drop with a threshold\n",
    "\n",
    "Count how many rows would be dropped if we only dropped rows that had a least 12 NON-Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ec8d56-c0de-4334-9ded-83cd0b8c62ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da7c79f-504c-49d6-9a93-250a21e6a6af",
     "showTitle": false,
     "title": ""
    },
    "id": "nUl-uY0IA2hD"
   },
   "source": [
    "## S3.5. Drop rows according to specific column value\n",
    "\n",
    "Now count how many rows would be dropped if you only drop rows whose values in the tempm column are null/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538e5fb2-cf4a-4852-b683-01d72a55049b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e711238-6337-44e6-990f-5ad0cfc611d7",
     "showTitle": false,
     "title": ""
    },
    "id": "s3WY5mZzA2hE"
   },
   "source": [
    "## S3.6. Drop rows that are null accross all columns\n",
    "\n",
    "Count how many rows would be dropped if you only dropped rows where ALL the values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b17197-a392-4fda-b47d-5f03a9dec065",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f560a9a-a08a-4641-81d9-4fd828a6f68e",
     "showTitle": false,
     "title": ""
    },
    "id": "IAZ2FDpAA2hF"
   },
   "source": [
    "## S3.7. Fill in all the string columns missing values with the word \"N/A\"\n",
    "\n",
    "Make sure you don't edit the df dataframe itself. Create a copy of the df then edit that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69852478-c44e-4c9e-8aee-c18166ac7afd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c11490-e1f0-46c4-a6f2-361cafc330c0",
     "showTitle": false,
     "title": ""
    },
    "id": "XP6Wf9XEA2hG"
   },
   "source": [
    "## S3.8. Fill in NaN values with averages for the tempm and tempi columns\n",
    "\n",
    "*Note: you will first need to compute the averages for each column and then fill in with the corresponding value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "907faac3-37cb-4be7-8b0c-0d70b61acf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e7a009a-acf5-4c09-ad08-29ec6a4deb5e",
     "showTitle": false,
     "title": ""
    },
    "id": "viCkM6m3A2hG"
   },
   "source": [
    "<h2 align='center'><b><font color='#3F13E2'>That's it! Great Job! </font></b></h2>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_exercise_1 ",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
