{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e66c2e86-4339-4690-8307-6536dd33f51a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div  align='center'><img src='https://s3.amazonaws.com/weclouddata/images/logos/wcd_logo_new_2.png' width='30%'></div >\n",
    "\n",
    "<p style=\"font-size:20px;text-align:center\"><b><font color='#F39A54'>Data Engineering Diploma</font></b></p>\n",
    "\n",
    "<h2 align='center'> WeCloudData Data Engineer Spark Exercise 1 </h2>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35a8f47-5aba-4fec-a7d5-6f9c53d62a44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Please download data from [HERE](s3://weclouddata/data/data/pyspark_exercises_data.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72d8089-de33-402b-90c8-7feec8ed8361",
     "showTitle": false,
     "title": ""
    },
    "id": "p9rHQVQXA2gV"
   },
   "source": [
    "# Section 1. Reading, Writing and Validating Data in PySpark HW Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a73e88-0306-4876-a9dd-892e5c0ff114",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "WGm9tdBeA2gX",
    "outputId": "fb991d62-50ed-4278-c686-6f5ac8a0cf0c"
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"ReadWriteVal\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d1de6c-e444-4bf2-8c99-b39dabd160d2",
     "showTitle": false,
     "title": ""
    },
    "id": "g_12i0vsA2ga"
   },
   "source": [
    "## Next let's start by reading a basic csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b478ed23-ecbc-4721-a73b-42ce4d78592e",
     "showTitle": false,
     "title": ""
    },
    "id": "3eufR0XoA2ga"
   },
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "\n",
    "# Some csv data\n",
    "pga = spark.read.csv(path+'pga_tour_historical.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55753829-4749-426a-bc4c-38f9a651abcd",
     "showTitle": false,
     "title": ""
    },
    "id": "33btaZAPA2gc"
   },
   "source": [
    "## S1.1. View first 5 lines of dataframe\n",
    "First generate a view of the first 5 lines of the dataframe to get an idea of what is inside. We went over two ways of doing this... see if you can remember BOTH ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca7b4a9-ece7-45fa-a53a-70435de2d8d4",
     "showTitle": false,
     "title": ""
    },
    "id": "tW4EEELhA2gc",
    "outputId": "5e2446ca-1e9b-45d0-9cca-6514345bf922"
   },
   "outputs": [],
   "source": [
    "pga.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce1c32d-6a7b-4f6c-aa7d-36f4d8477311",
     "showTitle": false,
     "title": ""
    },
    "id": "4h4emUtnA2gd",
    "outputId": "ad9e9f1f-e287-4b58-d7ae-aee5f67881fe"
   },
   "outputs": [],
   "source": [
    "# I prefer this method\n",
    "pga.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f058a9-9816-4423-a388-87590d2b68eb",
     "showTitle": false,
     "title": ""
    },
    "id": "rpLiYlvSA2ge"
   },
   "source": [
    "## S1.2. Print the schema details\n",
    "\n",
    "Now print the details of the dataframes schema that Spark infered to ensure that it was infered correctly. Sometimes it is not infered correctly, so we need to watch out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647fac47-69fc-43e7-8a83-1de128dec332",
     "showTitle": false,
     "title": ""
    },
    "id": "zjp35r2DA2ge",
    "outputId": "43a7c7a6-4e71-4cbd-a6c2-e7d35f1f29f9"
   },
   "outputs": [],
   "source": [
    "print(pga.printSchema())\n",
    "print(\"\")\n",
    "print(pga.columns)\n",
    "print(\"\")\n",
    "# Not so fond of this method, but to each their own\n",
    "print(pga.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83646cb2-330c-4751-bbbf-2bfa8a749945",
     "showTitle": false,
     "title": ""
    },
    "id": "uZgyg7z9A2gf"
   },
   "source": [
    "## S1.3. Edit the schema during the read in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36a3040b-b6e3-4267-b41f-8e91521fc0e0",
     "showTitle": false,
     "title": ""
    },
    "id": "EqEF2tlUA2gg"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e66b9ff-59ed-43b8-8a24-d80f9b179635",
     "showTitle": false,
     "title": ""
    },
    "id": "lcoGkV5yA2gg"
   },
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"Player Name\", StringType(), True), \\\n",
    "               StructField(\"Season\", IntegerType(), True), \\\n",
    "               StructField(\"Statistic\", StringType(), True), \\\n",
    "               StructField(\"Variable\", StringType(), True), \\\n",
    "               StructField(\"Value\", IntegerType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd122475-b5e3-47b3-aae6-55dd3cebe3b3",
     "showTitle": false,
     "title": ""
    },
    "id": "hHkVuwIoA2gh"
   },
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64593d93-47c4-41fe-9d89-3105e988f01e",
     "showTitle": false,
     "title": ""
    },
    "id": "l5HaKbDNA2gh"
   },
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "pga = spark.read.csv(path+'pga_tour_historical.csv', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffeb0363-34d1-4456-9165-c09aec15d277",
     "showTitle": false,
     "title": ""
    },
    "id": "_dSS-GhlA2gi",
    "outputId": "598fd80a-b1c6-4711-8605-b4b2f7a3bc17"
   },
   "outputs": [],
   "source": [
    "pga.printSchema()\n",
    "# That's better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e67870c-c4a2-4315-892e-dc104b2c81f7",
     "showTitle": false,
     "title": ""
    },
    "id": "7yjcI60kA2gj"
   },
   "source": [
    "## S1.4. Generate summary statistics for only one variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b625cfe-1703-49bd-86a2-0f5234206d59",
     "showTitle": false,
     "title": ""
    },
    "id": "Lq3qbfc0A2gj",
    "outputId": "3b7bfb35-9383-44d2-a9de-fc9d95ba21d7"
   },
   "outputs": [],
   "source": [
    "# Neat \"describe\" function\n",
    "pga.describe(['Value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73de969-292a-4edc-8904-0cfbc7dba297",
     "showTitle": false,
     "title": ""
    },
    "id": "nN2UCIWXA2gk"
   },
   "source": [
    "## S1.5. Generate summary statistics for TWO variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4290be33-415c-4532-977e-e464431f0044",
     "showTitle": false,
     "title": ""
    },
    "id": "TFr4lhwZA2gl",
    "outputId": "8d6c9a80-0b9c-4d56-a201-c8aec51d8590"
   },
   "outputs": [],
   "source": [
    "pga.select(\"Season\", \"Value\").summary(\"count\", \"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19b3b415-9d27-4981-8e04-ed158c9c235b",
     "showTitle": false,
     "title": ""
    },
    "id": "nhJ3aRXoA2gl"
   },
   "source": [
    "## S1.6. Write a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37192fdc-19df-4e97-8b43-d8775ac9f07b",
     "showTitle": false,
     "title": ""
    },
    "id": "dpbcVe_8A2gm"
   },
   "outputs": [],
   "source": [
    "df = pga.select(\"Season\",\"Value\")\n",
    "df.write.mode(\"overwrite\").parquet(\"partition_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0362f221-e91c-48ce-b5c9-4fff765e41f8",
     "showTitle": false,
     "title": ""
    },
    "id": "iMCN1CRWA2gm"
   },
   "source": [
    "## S1.7. Write a partitioned parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee1cc2d-2676-4b2a-b4ae-844a98862b43",
     "showTitle": false,
     "title": ""
    },
    "id": "0mVvgI0sA2gm",
    "outputId": "be47895f-2f76-4f94-c80d-9058cd4d25d3"
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"Season\").parquet(\"partitioned_parquet/\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17bb9434-526f-4116-a065-607ad3697450",
     "showTitle": false,
     "title": ""
    },
    "id": "hj7-W-nDA2gn"
   },
   "source": [
    "## S1.8. Read in a partitioned parquet file\n",
    "\n",
    "Now try reading in the partitioned parquet file you just created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f41a49-027f-4a8c-89cf-1b5e5d8fe5eb",
     "showTitle": false,
     "title": ""
    },
    "id": "J3tsSrNXA2gn",
    "outputId": "66ec746d-a916-404f-ed8b-aa2f67ef66e4"
   },
   "outputs": [],
   "source": [
    "path = \"partitioned_parquet/\" #Note: if you add a * to the end of the path, the Season var will be automatically dropped\n",
    "parquet = spark.read.parquet(path)\n",
    "        \n",
    "parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78e69e36-7aeb-42ae-8a63-b92f7fa276b6",
     "showTitle": false,
     "title": ""
    },
    "id": "X6PIYDuUA2go"
   },
   "source": [
    "## S1.9. Reading in a set of paritioned parquet files\n",
    "\n",
    "Now try only reading Seasons 2010, 2011 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4963969c-dd01-4dec-9f3b-0ac648df2f60",
     "showTitle": false,
     "title": ""
    },
    "id": "GX4zpG5CA2go",
    "outputId": "f725e942-0b59-4b93-8c34-191a3302344e"
   },
   "outputs": [],
   "source": [
    "# Notice that this method only gives you the \"Value\" column\n",
    "path = \"partitioned_parquet/\"\n",
    "partitioned = spark.read.parquet(path+'Season=2010/',\\\n",
    "                             path+'Season=2011/', \\\n",
    "                             path+'Season=2012/')\n",
    "\n",
    "partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d08f83-c3d4-4c85-99fa-f8a1324c1707",
     "showTitle": false,
     "title": ""
    },
    "id": "1vShVzDvA2gp",
    "outputId": "caad16ff-3253-426d-ae0a-3d529f41c559"
   },
   "outputs": [],
   "source": [
    "# We need to use this method to get the \"Season\" and \"Value\" Columns\n",
    "path = \"partitioned_parquet/\"\n",
    "dataframe = spark.read.option(\"basePath\", path).parquet(path+'Season=2010/',\\\n",
    "                                                                path+'Season=2011/', \\\n",
    "                                                                path+'Season=2012/')\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6a804a-245e-4f2e-8e99-039485031fa7",
     "showTitle": false,
     "title": ""
    },
    "id": "to1ULOa-A2gp"
   },
   "source": [
    "## S1.10. Create your own dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbc468f-8664-408c-be82-2368425ed9d4",
     "showTitle": false,
     "title": ""
    },
    "id": "SZSi6mU-A2gp",
    "outputId": "766a0037-fa44-4113-c34d-354545177d08"
   },
   "outputs": [],
   "source": [
    "values = [('Kyle',10,'A',1),('Melbourne',36,'A',1),('Nina',123,'A',1),('Stephen',48,'B',2),('Orphan',16,'B',2),('Imran',1,'B',2)]\n",
    "df = spark.createDataFrame(values,['name','age','AB','Number'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b066c485-8f70-4794-bc3f-c0c64ea40427",
     "showTitle": false,
     "title": ""
    },
    "id": "33iY5t9NA2gq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e21547c-dc8a-40de-a3ed-4e1ef925e0b5",
     "showTitle": false,
     "title": ""
    },
    "id": "vF0inwX4A2gq"
   },
   "source": [
    "# Section 2. Manipulating Data in DataFrames Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c050008-f763-451c-a5df-90c5364692ef",
     "showTitle": false,
     "title": ""
    },
    "id": "43fNmAeVA2gq",
    "outputId": "e23e0aa1-a3d6-4965-d0c4-b304764922ac"
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Manip\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "610324c9-3bd9-448a-b67f-552818a4486c",
     "showTitle": false,
     "title": ""
    },
    "id": "r70W8E7dA2gr"
   },
   "source": [
    "## Read in our Republican vs. Democrats Tweet DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d7faca-20ff-4e9b-b1ff-dc07721c542f",
     "showTitle": false,
     "title": ""
    },
    "id": "cfmRt0deA2gs"
   },
   "outputs": [],
   "source": [
    "path='Datasets/'\n",
    "tweets = spark.read.csv(path+'Rep_vs_Dem_tweets.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b603f343-9233-4fc7-814d-e63cfcfc427d",
     "showTitle": false,
     "title": ""
    },
    "id": "zM2QGFC7A2gs",
    "outputId": "6a527a47-b924-4f69-feaa-5d5f7ba2bae1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a445d410-3077-4198-9b8c-cd87d0d548b3",
     "showTitle": false,
     "title": ""
    },
    "id": "WHM78z3dA2gt"
   },
   "source": [
    "**Prevent Truncation of view**\n",
    "\n",
    "If the view you produced above truncated some of the longer tweets, see if you can prevent that so you can read the whole tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d16fd9-9e05-4292-91c3-9704d4bc6ac3",
     "showTitle": false,
     "title": ""
    },
    "id": "dlEB-lgkA2gt",
    "outputId": "efcc8510-b012-4527-8e4c-af88f3308241"
   },
   "outputs": [],
   "source": [
    "tweets.select(\"tweet\").show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf074dbd-dc47-4b0d-b48a-1a1ba136fa62",
     "showTitle": false,
     "title": ""
    },
    "id": "lh7bTNlrA2gt"
   },
   "source": [
    "As we can see, this dataset contains three columns. The tweet content, Twitter handle that tweeted the tweet, and the party that that tweet belongs to. But it looks like the tweets could use some cleaning, esspecially if we are going to this for some kind of machine learning analysis. Let's see if we can make this an even richer dataset using the techniques we learned in the lecture!\n",
    "\n",
    "**Print Schema**\n",
    "\n",
    "First, check the schema to make sure the datatypes are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ce6f36-4f39-489f-9d13-f451a9a8ef8a",
     "showTitle": false,
     "title": ""
    },
    "id": "2EmuJ5DpA2gu",
    "outputId": "9cdf4a54-fc8f-4885-f77e-3255f8ccf1ed"
   },
   "outputs": [],
   "source": [
    "print(tweets.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "242b504f-111d-42a3-b963-ea442de66172",
     "showTitle": false,
     "title": ""
    },
    "id": "_dQLhQZOA2gu"
   },
   "source": [
    "## S2.1. Can you identify any tweet that mentions the handle @LatinoLeader using regexp_extract?\n",
    "\n",
    "It doesn't matter how you identify the row, any identifier will do. You can test your script on row 5 from this dataset. That row contains @LatinoLeader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb36ba5-b9ae-47ca-8ae8-863a715209b0",
     "showTitle": false,
     "title": ""
    },
    "id": "FtOChedGA2gu",
    "outputId": "fb945336-0372-4060-ef16-0dae8bb78105"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * #regexp_extract\n",
    "latino = tweets.withColumn('Latino_Mentions',regexp_extract(tweets.Tweet, '(.)(@LatinoLeader)(.)',2))\n",
    "latino.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b346bf0d-4fd5-4652-862e-178084da3b36",
     "showTitle": false,
     "title": ""
    },
    "id": "eqyTvWaCA2gv"
   },
   "source": [
    "## S2.2. Replace any value other than 'Democrate' or 'Republican' with 'Other' in the Party column.\n",
    "\n",
    "We can see from the output below, that there are several other values other than 'Democrate' or 'Republican' in the Part column. We are assuming that this is dirty data that needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ab911d-5379-43d9-88fb-091f9b831c0a",
     "showTitle": false,
     "title": ""
    },
    "id": "WJIV9e_5A2gv",
    "outputId": "4b5d35ca-a8d5-4cae-b665-405fd8f7a9bc"
   },
   "outputs": [],
   "source": [
    "# We haven't gotten to this yet so it's a bit of a teaser :)\n",
    "from pyspark.sql.functions import *\n",
    "counts = tweets.groupBy(\"Party\").count()\n",
    "counts.orderBy(desc(\"count\")).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264407be-c2de-4004-a047-022ea464c8e0",
     "showTitle": false,
     "title": ""
    },
    "id": "gxsKqbjKA2gw",
    "outputId": "f58b2e12-8ed5-49f0-c2ed-34b765f1f93d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "clean = tweets.withColumn('Party', when(tweets.Party == 'Democrat', 'Democrat').when(tweets.Party == 'Republican', 'Republican').otherwise('Other'))\n",
    "counts = clean.groupBy(\"Party\").count()\n",
    "counts.orderBy(desc(\"count\")).show(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f63e1d-48ca-4a00-a7bd-e64a354a2af8",
     "showTitle": false,
     "title": ""
    },
    "id": "dyK1rUWvA2gw"
   },
   "source": [
    "## S2.3. Delete all embedded links (ie. \"https:....)\n",
    "\n",
    "For example see the first row in the tweets dataframe. \n",
    "\n",
    "*Note: this may require an google search :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0f7827-3c26-453b-9ebb-e9e0cc30300a",
     "showTitle": false,
     "title": ""
    },
    "id": "XD7miBMDA2gw",
    "outputId": "ba1b0ee2-3f0f-4b74-b0a9-1a8a82ce4886"
   },
   "outputs": [],
   "source": [
    "print(\"OG Tweet\")\n",
    "tweets.select(\"tweet\").show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "453a0059-084e-45ec-8150-19dcb069a372",
     "showTitle": false,
     "title": ""
    },
    "id": "8Ks6LnhQA2gx",
    "outputId": "6a094901-db93-42ae-d840-87310ffae0b5"
   },
   "outputs": [],
   "source": [
    "# And here is the solution\n",
    "print(\"Cleaned Tweet\")\n",
    "tweets.withColumn('cleaned', regexp_replace('Tweet', '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '')).select(\"cleaned\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659a1340-5c4c-43dd-bd35-a8fc63af3420",
     "showTitle": false,
     "title": ""
    },
    "id": "yuKE3lw2A2gx"
   },
   "source": [
    "## S2.4. Remove any leading or trailing white space in the tweet column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a37ffdcd-e399-4545-b8e2-ed1e66709d49",
     "showTitle": false,
     "title": ""
    },
    "id": "7YXq7ECKA2gy",
    "outputId": "0ec7d1f5-a522-4fcd-ed45-d556ae8ca4ea"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "tweets.select(\"Tweet\").show(5,False)\n",
    "tweets.select('Tweet', trim(tweets.Tweet)).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a5f820-2e6e-405e-b0c6-71c88e579afb",
     "showTitle": false,
     "title": ""
    },
    "id": "7P7_QAZRA2gy"
   },
   "source": [
    "## S2.5. Rename the 'Party' column to 'Dem_Rep'\n",
    "\n",
    "No real reason here :) just wanted you to get practice doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30132710-a13d-4b17-843a-a40327c37c16",
     "showTitle": false,
     "title": ""
    },
    "id": "ejrSHFE7A2gy",
    "outputId": "4acc820f-11d4-4ac1-a8f9-85067cdd5ab1"
   },
   "outputs": [],
   "source": [
    "renamed = tweets.withColumnRenamed('Party','Dem_Rep')\n",
    "renamed.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0e4aa9-3aa3-43cc-8331-28289e7d5887",
     "showTitle": false,
     "title": ""
    },
    "id": "V5AO-dYLA2gz"
   },
   "source": [
    "## S2.6. Concatenate the Party and Handle columns\n",
    "\n",
    "Silly yes... but good practice.\n",
    "\n",
    "pyspark.sql.functions.concat_ws(sep, *cols)[source](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.concat_ws.html)\n",
    "\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b2640d-94f8-4393-bb17-692daec08ced",
     "showTitle": false,
     "title": ""
    },
    "id": "bINDCOnBA2gz",
    "outputId": "ab390394-3d4e-4c49-d536-dc458d1650f0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "tweets.select(tweets.Party,tweets.Handle,concat_ws(' ', tweets.Party, tweets.Handle).alias('Concatenated')).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2d42de-c846-4d27-a3ad-e07cd39daedd",
     "showTitle": false,
     "title": ""
    },
    "id": "jBsqSTAdA2g0"
   },
   "source": [
    "## Challenge Question\n",
    "\n",
    "Let's image that we want to analyze the hashtags that are used in these tweets. Can you extract all the hashtags you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f80d21-4665-4b6f-9d12-41b65507b550",
     "showTitle": false,
     "title": ""
    },
    "id": "dMzH7sT0A2g0",
    "outputId": "5298d138-9525-47d5-e709-b6c7f4859113"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Parenthesis are used to mark a subexpression within a larger expression\n",
    "# The . matches any character other than a new line\n",
    "# | means is like or\n",
    "# \\w+ means followed by any word\n",
    "pattern = '(.|'')(#)(\\w+)'\n",
    "# * is used to match the preceding character zero or more times.\n",
    "# ? will match the preceding character zero or one times, but no more.\n",
    "# $ is used to match the ending position in a string. \n",
    "split_pattern = r'.*?({pattern})'.format(pattern=pattern)\n",
    "end_pattern = r'(.*{pattern}).*?$'.format(pattern=pattern)\n",
    "\n",
    "# $1 here means to capture the first part of the regex result\n",
    "# The , will separate each find with a comma in the a array we create\n",
    "df2 = tweets.withColumn('a', regexp_replace('Tweet', split_pattern, '$1,')).where(col('Tweet').like('%#%'))\n",
    "df2.select('a').show(3,False)\n",
    "# Remove all the other results that came up\n",
    "df3 = df2.withColumn('a', regexp_replace('a', end_pattern, '$1'))\n",
    "df3.select('a').show(3,False)\n",
    "# Finally create an array from the result by splitting on the comma\n",
    "df4 = df3.withColumn('a', split('a', r','))\n",
    "df4.select('a').show(3,False)\n",
    "df4.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ed97df-8814-4db0-98c3-2b2211904b66",
     "showTitle": false,
     "title": ""
    },
    "id": "5SB47NlpA2g0"
   },
   "source": [
    "## Let's create our own dataset to work with real dates\n",
    "\n",
    "This is a dataset of patient visits from a medical office. It contains the patients first and last names, date of birth, and the dates of their first 3 visits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f94730-c470-4ee8-a9dd-bb673dfd926a",
     "showTitle": false,
     "title": ""
    },
    "id": "1gXI6mmxA2g1",
    "outputId": "cd6e3928-8e99-4254-ee92-dbff069dfc52"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "md_office = [('Mohammed','Alfasy','1987-4-8','2016-1-7','2017-2-3','2018-3-2') \\\n",
    "            ,('Marcy','Wellmaker','1986-4-8','2015-1-7','2017-1-3','2018-1-2') \\\n",
    "            ,('Ginny','Ginger','1986-7-10','2014-8-7','2015-2-3','2016-3-2') \\\n",
    "            ,('Vijay','Doberson','1988-5-2','2016-1-7','2018-2-3','2018-3-2') \\\n",
    "            ,('Orhan','Gelicek','1987-5-11','2016-5-7','2017-1-3','2018-9-2') \\\n",
    "            ,('Sarah','Jones','1956-7-6','2016-4-7','2017-8-3','2018-10-2') \\\n",
    "            ,('John','Johnson','2017-10-12','2018-1-2','2018-10-3','2018-3-2') ]\n",
    "\n",
    "df = spark.createDataFrame(md_office,['first_name','last_name','dob','visit1','visit2','visit3']) # schema=final_struc\n",
    "\n",
    "# Check to make sure it worked\n",
    "df.show()\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efae92ca-67eb-4bd6-a723-ea9153ea1a84",
     "showTitle": false,
     "title": ""
    },
    "id": "tSsQXDdIA2g1"
   },
   "source": [
    "Ooops! The dates are still stored as text... let's try converting them again and see if we have any issues this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0501cbef-eb08-45bc-8eae-7e6fe3f2fd4d",
     "showTitle": false,
     "title": ""
    },
    "id": "FdnDFDv3A2g1",
    "outputId": "5a0e535e-bc9c-4e65-8ce5-d55acaac33b9"
   },
   "outputs": [],
   "source": [
    "# Covert the date columns into date types\n",
    "df = df.withColumn(\"dob\", df[\"dob\"].cast(DateType())) \\\n",
    "        .withColumn(\"visit1\", df[\"visit1\"].cast(DateType())) \\\n",
    "        .withColumn(\"visit2\", df[\"visit2\"].cast(DateType())) \\\n",
    "        .withColumn(\"visit3\", df[\"visit3\"].cast(DateType()))\n",
    "\n",
    "# Check to make sure it worked\n",
    "df.show()\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ae993f-8140-4417-8c07-0b61d0048b54",
     "showTitle": false,
     "title": ""
    },
    "id": "x499Xq8EA2g2"
   },
   "source": [
    "## S2.7. Can you calculate a variable showing the length of time between patient visits?\n",
    "\n",
    "Compare visit1 to visit2 and visit2 to visit3 for all patients and see what the average length of time is between visits. Create an alias for it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab0b895-8934-423c-826c-3ec90cbf5410",
     "showTitle": false,
     "title": ""
    },
    "id": "IV-4usQhA2g2",
    "outputId": "d38600ec-55fb-45ab-bcbc-20d418c23053"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "diff1 = df.select(datediff(df.visit2, df.visit1).alias(\"diff\"))\n",
    "diff2 = df.select(datediff(df.visit3, df.visit2).alias(\"diff\"))\n",
    "\n",
    "# Append the two dataframes together\n",
    "diff_combo = diff1.union(diff2)\n",
    "diff_combo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9df8194a-959d-4bb8-8d9c-f3023b633377",
     "showTitle": false,
     "title": ""
    },
    "id": "kwg19-gAA2g2"
   },
   "source": [
    "## S2.8. Can you calculate the age of each patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6990042-14f4-4ce2-bc85-7c8906af1fd1",
     "showTitle": false,
     "title": ""
    },
    "id": "z6vmcUDHA2g2",
    "outputId": "90d09569-412d-4b8d-8c0b-70c826c912de"
   },
   "outputs": [],
   "source": [
    "# We use the datediff function here as well\n",
    "# And divide by 365 to get the age\n",
    "# I also formated this value to get rid of all the decimal places\n",
    "ages = df.select(format_number(datediff(df.visit1,df.dob)/365,1).alias(\"age\"))\n",
    "ages.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eec2075e-4b76-44bc-9f5e-c5661a57c189",
     "showTitle": false,
     "title": ""
    },
    "id": "dmAm15YBA2g3"
   },
   "source": [
    "## S2.9. Can you extract the month from the first visit column and call it \"Month\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d16fc7fa-237a-416e-a786-d21d5ddc4591",
     "showTitle": false,
     "title": ""
    },
    "id": "1dAYz1Q9A2g3",
    "outputId": "3496a8e5-b60e-432b-e97b-8ad939a413c0"
   },
   "outputs": [],
   "source": [
    "month1 = df.select(month(df['visit1']).alias(\"Month\"))\n",
    "month1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2d1f64d-db09-4746-9683-6c0f4f28ded0",
     "showTitle": false,
     "title": ""
    },
    "id": "8TO1tDunA2g4",
    "outputId": "5757f638-12aa-405d-cc24-a6ab136a5e3d"
   },
   "outputs": [],
   "source": [
    "# Bonus (not in lecture)\n",
    "# If you wanted to make a list (or an array in this case) for all months, you could do this\n",
    "\n",
    "df.select(array(month(df['visit1']),month(df['visit2'])).alias(\"Months\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c17c60b2-9ce1-4ed3-87a5-fa44820c8bb8",
     "showTitle": false,
     "title": ""
    },
    "id": "8yDg6BERA2g4",
    "outputId": "980dceb0-ec1c-44cb-bae6-6414ef4adba7"
   },
   "outputs": [],
   "source": [
    "# Bonus (not in lecture)\n",
    "# Or even a separate col for each month\n",
    "\n",
    "df.select('*',month(df['visit1']).alias(\"Month1\"),month(df['visit2']).alias(\"Month2\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46362b70-bf1f-49aa-8f07-caa46e5d4d87",
     "showTitle": false,
     "title": ""
    },
    "id": "5RQ2T1zfA2g4",
    "outputId": "e137c230-4392-4d78-a7d1-67eb26a08282"
   },
   "outputs": [],
   "source": [
    "# Bonus (not in lecture)\n",
    "# Or loop over all visit columns\n",
    "\n",
    "# Get all visit column names\n",
    "df_month_cols = [i for i in df.columns if i.startswith('visit')]\n",
    "\n",
    "# Make a copy of our df\n",
    "df2 = df\n",
    "\n",
    "# Loop over relevant columns and add on month columns\n",
    "for column in df_month_cols:\n",
    "    # Find number of visit (this is straght up python, we don't need pyspark for this)\n",
    "    num = str(column)[-1]\n",
    "    # Create the naming convention for the new column  (python too)\n",
    "    new_col_name = \"Month\" + num\n",
    "    df2 = df2.withColumn(new_col_name,month(df[column]))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f3d3c7-3d27-43f4-b113-da9bd41cf491",
     "showTitle": false,
     "title": ""
    },
    "id": "qLAGGIB9A2g5"
   },
   "source": [
    "## S2.10. Challenges with working with date and timestamps\n",
    "\n",
    "Let's read in our supermarket sales dataframe and see some of the issues that can come up when working with date and timestamps values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b11f2fb9-8d5f-4306-b8ee-c631f95eb6ea",
     "showTitle": false,
     "title": ""
    },
    "id": "uDSpJgrRA2g5"
   },
   "outputs": [],
   "source": [
    "path = 'Datasets/'\n",
    "sales = spark.read.csv(path+'supermarket_sales.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "875ef2d7-08d9-4695-adec-b0dcd19d5426",
     "showTitle": false,
     "title": ""
    },
    "id": "Ex4LFdEWA2g5",
    "outputId": "8dfe45a8-a236-4c68-f3a9-b661011129d6"
   },
   "outputs": [],
   "source": [
    "sales.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5dc9690-341b-41fc-be4c-be98c94275e5",
     "showTitle": false,
     "title": ""
    },
    "id": "I8erM8sFA2g6",
    "outputId": "04c3129c-e99c-4a31-f631-d558ca54b6dc"
   },
   "outputs": [],
   "source": [
    "print(sales.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a823a2b-24cd-495d-b089-3a0f1aa8b3e3",
     "showTitle": false,
     "title": ""
    },
    "id": "3FlOTpG_A2g6"
   },
   "source": [
    "Looks like we need to convert the date field into a date type. Let's go ahead and do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5338449c-4620-4b4f-8fee-b393386a6bb7",
     "showTitle": false,
     "title": ""
    },
    "id": "NnJ3DQyEA2g6",
    "outputId": "3641d18d-732e-484b-8e4f-52b5c3bfeb64"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Note that this method gives all null values\")\n",
    "df = sales.withColumn(\"Formatted Date\", sales[\"Date\"].cast(DateType()))\n",
    "df = df.select(\"Date\",\"Formatted Date\")\n",
    "print(df.limit(6).toPandas())\n",
    "\n",
    "print(\" \")\n",
    "print(\"This result gives the wrong results (notice that all months are january)\")\n",
    "sales.select('Date',to_date(sales.Date, 'm/d/yyyy').alias('Dateformatted'),month(to_date(sales.Date, 'm/d/yyyy')).alias('Month'),).show(5)\n",
    "\n",
    "print(\" \")\n",
    "print(\"But if we capitalize the mm part in the format, we get the correct results!\")\n",
    "sales.select('Date',to_date(sales.Date, 'M/d/yyyy').alias('Dateformatted'),month(to_date(sales.Date, 'M/d/yyyy')).alias('Month'),).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05cf0e8-667e-4eb1-9a44-1cc195df2ad3",
     "showTitle": false,
     "title": ""
    },
    "id": "2bUnhuktA2g7"
   },
   "source": [
    "## Another way we can extract the month value from the date field\n",
    "\n",
    "If your date format is uncommon, or you are not getting the expected results from the output above, you could also use this method to get what you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f370755-5962-4d40-ab9e-9788c0722c63",
     "showTitle": false,
     "title": ""
    },
    "id": "1Bu0zCSRA2g7",
    "outputId": "eec1b6a0-71f3-4492-925d-f058e4df5111"
   },
   "outputs": [],
   "source": [
    "# We need to creative here\n",
    "# First split the date field and get the month value \n",
    "df = sales.select('Date',split(sales.Date, '/')[0].alias('Month'),'Total')\n",
    "\n",
    "# Verify everything worked correctly\n",
    "print(\"Verify\")\n",
    "df.show(5)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee71e83c-12e0-42f1-b57e-b2747a4e61bb",
     "showTitle": false,
     "title": ""
    },
    "id": "BUaNHlgvA2g7"
   },
   "source": [
    "## S2.11.0 Working with Arrays\n",
    "\n",
    "Here is a dataframe of reviews from the movie the Dark Night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afa5c85-db61-4c17-961f-2e42dfc4c8af",
     "showTitle": false,
     "title": ""
    },
    "id": "80k-IdlAA2g8",
    "outputId": "18da3dd5-68ec-4e82-c6ec-4d5d256c5d6f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "values = [(5,'Epic. This is the best movie I have EVER seen'), \\\n",
    "          (4,'Pretty good, but I would have liked to seen better special effects'), \\\n",
    "          (3,'So so. Casting could have been improved'), \\\n",
    "          (5,'The most EPIC movie of the year! Casting was awesome. Special effects were so intense.'), \\\n",
    "          (4,'Solid but I would have liked to see more of the love story'), \\\n",
    "          (5,'THE BOMB!!!!!!!')]\n",
    "reviews = spark.createDataFrame(values,['rating', 'review_txt'])\n",
    "\n",
    "reviews.show(6,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab854db-8816-4043-aa25-8b728be71436",
     "showTitle": false,
     "title": ""
    },
    "id": "Y40EFu3qA2g8"
   },
   "source": [
    "## S2.11.1 Let's see if we can create an array off of the review text column and then derive some meaningful results from it.\n",
    "\n",
    "**But first** we need to clean the rview_txt column to make sure we can get what we need from our analysis later on. So let's do the following:\n",
    "\n",
    "1. Remove all punctuation\n",
    "2. lower case everything\n",
    "3. Remove white space (trim)\n",
    "3. Then finally, split the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61639156-91d2-4783-af60-1db0db62614c",
     "showTitle": false,
     "title": ""
    },
    "id": "TpvSBA7cA2g8",
    "outputId": "b687c9cd-a1cc-42fb-b7d2-fd275c326613"
   },
   "outputs": [],
   "source": [
    "# We can do 1-3 in one call here\n",
    "df = reviews.withColumn(\"cleaned_reviews\", trim(lower(regexp_replace(col('review_txt'),'[^\\sa-zA-Z0-9]', ''))))\n",
    "df.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84043d9-b9f6-40e2-ad2d-c005837042f6",
     "showTitle": false,
     "title": ""
    },
    "id": "9UuKZImcA2g9",
    "outputId": "3c7518b0-b431-4da0-ea5f-538a109d5a95"
   },
   "outputs": [],
   "source": [
    "# Then split on the spaces!\n",
    "df = df.withColumn(\"review_txt_array\", split(col(\"cleaned_reviews\"), \" \"))\n",
    "df.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00f1ebcc-df26-47fa-bb18-e6bed2eba4aa",
     "showTitle": false,
     "title": ""
    },
    "id": "2cg5hCSJA2g9"
   },
   "source": [
    "## S2.11.2 Alright now let's see if we can find which reviews contain the word 'Epic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b933c477-d24d-4354-a22e-cb5562e852ef",
     "showTitle": false,
     "title": ""
    },
    "id": "xJE97VR6A2g9",
    "outputId": "66994e00-2970-425c-98c3-d8719d9e8f44"
   },
   "outputs": [],
   "source": [
    "epic = df.withColumn(\"result\",array_contains(col(\"review_txt_array\"), \"epic\"))\n",
    "epic.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c592b49e-8029-4d15-9941-2af27d1512d9",
     "showTitle": false,
     "title": ""
    },
    "id": "3U15qoh2A2g-"
   },
   "source": [
    "# Section 3. Handling Missing Data in PySpark HW Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1df925c-8b74-46b7-a442-3cce58fa261f",
     "showTitle": false,
     "title": ""
    },
    "id": "fOKzG09BA2g-",
    "outputId": "39a4d26c-c500-4f15-8f6b-4f29437656f3"
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"nulls\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2644d6c7-eeee-4d7b-b333-67967044eb24",
     "showTitle": false,
     "title": ""
    },
    "id": "ljYy8sXzA2g_"
   },
   "source": [
    "## Read in the dataset for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b288438-3950-45ea-abbf-72d4272fca31",
     "showTitle": false,
     "title": ""
    },
    "id": "BpRgEA3rA2g_"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('Datasets/Weather.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7304ef5a-40a6-4380-b2fb-172a6d37ef25",
     "showTitle": false,
     "title": ""
    },
    "id": "r_NS6HTYA2g_"
   },
   "source": [
    "## About this dataset\n",
    "\n",
    "**New York City Taxi Trip - Hourly Weather Data**\n",
    "\n",
    "Here is some detailed weather data for the New York City Taxi Trips.\n",
    "\n",
    "**Source:** https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41316a69-bdcb-4975-b80b-6a20dd2528da",
     "showTitle": false,
     "title": ""
    },
    "id": "jLSTdRTbA2g_"
   },
   "source": [
    "### Print a view of the first several lines of the dataframe to see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a276ecf-9a93-4f2a-9a5a-3da78994a8a9",
     "showTitle": false,
     "title": ""
    },
    "id": "VnyiRM5YA2g_",
    "outputId": "198f9116-8026-412c-9f24-f183f799282b"
   },
   "outputs": [],
   "source": [
    "df.limit(8).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1174daa-fc3c-420d-9fd5-8b79d844c2b2",
     "showTitle": false,
     "title": ""
    },
    "id": "l4YwX25GA2hA"
   },
   "source": [
    "### Print the schema \n",
    "\n",
    "So that we can see if we need to make any corrections to the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5de5bd3c-1a8c-46ac-8c62-57e884cd8975",
     "showTitle": false,
     "title": ""
    },
    "id": "BfXkHaV4A2hA",
    "outputId": "9a73c425-61ad-442d-a1a1-68f082b6fc99"
   },
   "outputs": [],
   "source": [
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4346933c-5b56-4e07-b176-12a19b2137d5",
     "showTitle": false,
     "title": ""
    },
    "id": "GEJTRzJQA2hA"
   },
   "source": [
    "## S3.1. How much missing data are we working with?\n",
    "\n",
    "Get a count and percentage of each variable in the dataset to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb4cd6e-8760-445b-84ce-a98a577e2ff4",
     "showTitle": false,
     "title": ""
    },
    "id": "PuDc81BeA2hB",
    "outputId": "adb67433-4a4a-40de-ead8-16dab328d5be"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_With_Null_Value', 'Null_Values_Count','Null_Value_Percent']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "362be078-c243-45c8-b36a-5ac94ecff39e",
     "showTitle": false,
     "title": ""
    },
    "id": "OsYoNIvMA2hB"
   },
   "source": [
    "## S3.2. How many rows contain at least one null value?\n",
    "\n",
    "We want to know, if we use the df.ha option, how many rows will we loose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e08980e-050e-444b-a17b-46b04c960e2a",
     "showTitle": false,
     "title": ""
    },
    "id": "tbh3aoMUA2hB",
    "outputId": "632d0c18-e37b-4deb-bdf7-e19ba79c4164"
   },
   "outputs": [],
   "source": [
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows in the DF: \",og_len)\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7f3a66-0b8a-497b-92ba-83097c24f779",
     "showTitle": false,
     "title": ""
    },
    "id": "BMNLa_uzA2hB"
   },
   "source": [
    "Yikes! Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4f8793a-4ea9-4064-965f-49146708ba89",
     "showTitle": false,
     "title": ""
    },
    "id": "OlTSFZiIA2hC"
   },
   "source": [
    "## S3.3. Drop the missing data\n",
    "\n",
    "Drop any row that contains missing data across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee02212-a988-4a40-bf35-7509b13fa2cb",
     "showTitle": false,
     "title": ""
    },
    "id": "kxxHddheA2hC",
    "outputId": "61e0fc57-418f-47b1-9a17-e90a5c3b5147"
   },
   "outputs": [],
   "source": [
    "dropped = df.na.drop()\n",
    "dropped.limit(4).toPandas() \n",
    "\n",
    "# Note this statement is equivilant to the above:\n",
    "# zomato.na.drop(how='any').limit(4).toPandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15679a13-da53-4069-b12a-01a88a7ffc4e",
     "showTitle": false,
     "title": ""
    },
    "id": "K8BDYGgRA2hC"
   },
   "source": [
    "Yep, we have no more data :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4690051-c2b1-45e7-ba5f-93aa3c6923c5",
     "showTitle": false,
     "title": ""
    },
    "id": "V__PC30YA2hD"
   },
   "source": [
    "## S3.4. Drop with a threshold\n",
    "\n",
    "Count how many rows would be dropped if we only dropped rows that had a least 12 NON-Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c91e247-5e44-4e45-8826-8870e7b5236e",
     "showTitle": false,
     "title": ""
    },
    "id": "hx9sVFkfA2hD",
    "outputId": "ea771ced-bb1b-4be4-8c71-cdb28ab681d2"
   },
   "outputs": [],
   "source": [
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=12).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c43aa2-218c-45b6-89a6-28ed9379eff6",
     "showTitle": false,
     "title": ""
    },
    "id": "nUl-uY0IA2hD"
   },
   "source": [
    "## S3.5. Drop rows according to specific column value\n",
    "\n",
    "Now count how many rows would be dropped if you only drop rows whose values in the tempm column are null/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71cd8c88-cc7d-4fe9-97dc-93cc374c1986",
     "showTitle": false,
     "title": ""
    },
    "id": "fT_EnW9YA2hD",
    "outputId": "981ea03c-9c4b-40c0-ac70-eda1ca5ddce1"
   },
   "outputs": [],
   "source": [
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"tempm\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b068f94e-84c6-4557-833c-b34093607c0d",
     "showTitle": false,
     "title": ""
    },
    "id": "nrF94LKRA2hE",
    "outputId": "0db1d94c-3531-41fa-fc60-e2b708cfdadd"
   },
   "outputs": [],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(df.tempm.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd21e326-a98e-41dc-bbbf-048329cb8325",
     "showTitle": false,
     "title": ""
    },
    "id": "s3WY5mZzA2hE"
   },
   "source": [
    "## S3.6. Drop rows that are null accross all columns\n",
    "\n",
    "Count how many rows would be dropped if you only dropped rows where ALL the values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729af11b-a89f-4d15-affa-06465415b759",
     "showTitle": false,
     "title": ""
    },
    "id": "-gYU1SwoA2hE",
    "outputId": "9faaaf24-98c4-4aab-b7f7-b41ef520638c"
   },
   "outputs": [],
   "source": [
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcd3e9d-dc47-4cbe-be61-d4c1e1433041",
     "showTitle": false,
     "title": ""
    },
    "id": "IYizHtENA2hF"
   },
   "source": [
    "That's good news!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8472811f-4be6-4dcc-8eb3-c49de192c95a",
     "showTitle": false,
     "title": ""
    },
    "id": "IAZ2FDpAA2hF"
   },
   "source": [
    "## S3.7. Fill in all the string columns missing values with the word \"N/A\"\n",
    "\n",
    "Make sure you don't edit the df dataframe itself. Create a copy of the df then edit that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b67614-68f2-4fb7-97bb-5d4d65960fcf",
     "showTitle": false,
     "title": ""
    },
    "id": "rHpgYHbwA2hF",
    "outputId": "0c77e144-088c-48d1-d8c9-48d2d959b56e"
   },
   "outputs": [],
   "source": [
    "null_fill = df.na.fill('N/A')\n",
    "null_fill.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8cdf2c7-57fc-4607-9770-9244e38dbd22",
     "showTitle": false,
     "title": ""
    },
    "id": "XP6Wf9XEA2hG"
   },
   "source": [
    "## S3.8. Fill in NaN values with averages for the tempm and tempi columns\n",
    "\n",
    "*Note: you will first need to compute the averages for each column and then fill in with the corresponding value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d8cf620-cc79-4976-80bf-25ccdeefe7b2",
     "showTitle": false,
     "title": ""
    },
    "id": "q_Al47N0A2hG",
    "outputId": "7efbf04e-49e3-4566-a824-4394be0be3cc"
   },
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "#     stats = stats.select(*(col(c).cast(\"int\").alias(c) for c in stats.columns)) #IntegerType()\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"tempm\",\"tempi\"])\n",
    "updated_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c9d988-e629-4dc1-bd4a-185d591b2ae7",
     "showTitle": false,
     "title": ""
    },
    "id": "viCkM6m3A2hG"
   },
   "source": [
    "<h2 align='center'><b><font color='#3F13E2'>That's it! Great Job! </font></b></h2>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_exercise_1_solution",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
